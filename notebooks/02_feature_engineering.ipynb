{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading sales data...\")\n",
    "sales_data = pd.read_csv('../sales_data.csv')\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "print(\"\\n--- Data Preprocessing ---\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in the dataset: {sales_data.isnull().sum().sum()}\")\n",
    "\n",
    "# Sort data by time for proper sequence\n",
    "sales_data = sales_data.sort_values(['distributor_id', 'sku', 'time_idx'])\n",
    "\n",
    "print(\"\\n--- Feature Engineering for TFT ---\")\n",
    "print(\"Temporal Fusion Transformer requires specific feature categories:\")\n",
    "print(\"1. Static features - constant over time for a given entity\")\n",
    "print(\"2. Time-varying known features - known in advance for forecasting steps\")\n",
    "print(\"3. Time-varying unknown features - not known in advance for future periods\")\n",
    "print(\"4. Target variable - what we're trying to predict\")\n",
    "\n",
    "# 2. Feature Categorization\n",
    "\n",
    "# Static features (constant for each distributor-sku combination)\n",
    "static_features = ['distributor_id', 'industry', 'sku', 'category', 'movement_category']\n",
    "\n",
    "# Time-varying known features (known at forecast time)\n",
    "time_varying_known_features = [\n",
    "    'quarter', 'year', \n",
    "    'is_diwali', 'is_ganesh_chaturthi', 'is_gudi_padwa', 'is_eid',\n",
    "    'is_akshay_tritiya', 'is_dussehra_navratri', 'is_onam', 'is_christmas'\n",
    "]\n",
    "\n",
    "# Time-varying unknown features (not known at forecast time)\n",
    "time_varying_unknown_features = ['prev_quarter_sales', 'total_quarter_sales', 'avg_quarterly_sales']\n",
    "\n",
    "# Target variable\n",
    "target = 'sales'\n",
    "\n",
    "print(\"\\n--- Feature Counts ---\")\n",
    "print(f\"Static features: {len(static_features)}\")\n",
    "print(f\"Time-varying known features: {len(time_varying_known_features)}\")\n",
    "print(f\"Time-varying unknown features: {len(time_varying_unknown_features)}\")\n",
    "\n",
    "# 3. Feature Transformations\n",
    "\n",
    "# Create unique identifiers for each distributor-SKU pair\n",
    "sales_data['entity_id'] = sales_data['distributor_id'] + '_' + sales_data['sku']\n",
    "print(f\"\\nNumber of unique entities (distributor-SKU combinations): {sales_data['entity_id'].nunique()}\")\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for feature in static_features:\n",
    "    if sales_data[feature].dtype == 'object':\n",
    "        encoder = LabelEncoder()\n",
    "        sales_data[f\"{feature}_encoded\"] = encoder.fit_transform(sales_data[feature])\n",
    "        label_encoders[feature] = encoder\n",
    "        print(f\"Encoded {feature} with {len(encoder.classes_)} unique values\")\n",
    "\n",
    "# Create cyclical encoding for quarter\n",
    "sales_data['quarter_sin'] = np.sin(2 * np.pi * sales_data['quarter'] / 4)\n",
    "sales_data['quarter_cos'] = np.cos(2 * np.pi * sales_data['quarter'] / 4)\n",
    "\n",
    "# Scale numeric features\n",
    "numeric_features = ['prev_quarter_sales', 'total_quarter_sales', 'avg_quarterly_sales', 'sales']\n",
    "scaler = StandardScaler()\n",
    "sales_data[numeric_features] = scaler.fit_transform(sales_data[numeric_features])\n",
    "\n",
    "# Save scaler for inference\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/feature_scaler.joblib')\n",
    "\n",
    "# 4. Time Series Specific Feature Engineering\n",
    "\n",
    "# Calculate sales momentum (change from previous quarter)\n",
    "sales_data['sales_momentum'] = sales_data.groupby(['entity_id'])['sales'].diff()\n",
    "\n",
    "# Calculate average sales across all products for each distributor by quarter\n",
    "distributor_quarter_avg = sales_data.groupby(['distributor_id', 'year', 'quarter'])['sales'].transform('mean')\n",
    "sales_data['distributor_quarter_avg'] = distributor_quarter_avg\n",
    "\n",
    "# Calculate average sales across all distributors for each product by quarter\n",
    "product_quarter_avg = sales_data.groupby(['sku', 'year', 'quarter'])['sales'].transform('mean')\n",
    "sales_data['product_quarter_avg'] = product_quarter_avg\n",
    "\n",
    "# 5. Festival Impact Features\n",
    "\n",
    "# Create combined festival indicator\n",
    "sales_data['has_festival'] = (sales_data[['is_diwali', 'is_ganesh_chaturthi', 'is_gudi_padwa', 'is_eid',\n",
    "                                        'is_akshay_tritiya', 'is_dussehra_navratri', 'is_onam', \n",
    "                                        'is_christmas']].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "# Count festivals in each quarter\n",
    "sales_data['festival_count'] = sales_data[['is_diwali', 'is_ganesh_chaturthi', 'is_gudi_padwa', 'is_eid',\n",
    "                                         'is_akshay_tritiya', 'is_dussehra_navratri', 'is_onam', \n",
    "                                         'is_christmas']].sum(axis=1)\n",
    "\n",
    "# 6. Visualize Engineered Features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(sales_data[['sales', 'prev_quarter_sales', 'sales_momentum', \n",
    "                        'distributor_quarter_avg', 'product_quarter_avg',\n",
    "                        'festival_count']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Between Engineered Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Prepare Data for TFT Format\n",
    "\n",
    "# Group by entity and sort by time\n",
    "grouped_data = []\n",
    "for entity, group in sales_data.groupby('entity_id'):\n",
    "    group = group.sort_values('time_idx')\n",
    "    grouped_data.append(group)\n",
    "\n",
    "print(f\"\\nData grouped by {len(grouped_data)} unique entities\")\n",
    "\n",
    "# 8. Create train/validation/test splits\n",
    "# For time series data, we need to split by time rather than random sampling\n",
    "\n",
    "# Identify the time range\n",
    "min_time_idx = sales_data['time_idx'].min()\n",
    "max_time_idx = sales_data['time_idx'].max()\n",
    "time_range = max_time_idx - min_time_idx\n",
    "\n",
    "# Create splits (60%/20%/20%)\n",
    "train_cutoff = min_time_idx + int(time_range * 0.6)\n",
    "val_cutoff = min_time_idx + int(time_range * 0.8)\n",
    "\n",
    "train_data = sales_data[sales_data['time_idx'] <= train_cutoff].copy()\n",
    "val_data = sales_data[(sales_data['time_idx'] > train_cutoff) & \n",
    "                       (sales_data['time_idx'] <= val_cutoff)].copy()\n",
    "test_data = sales_data[sales_data['time_idx'] > val_cutoff].copy()\n",
    "\n",
    "print(\"\\n--- Data Splits ---\")\n",
    "print(f\"Training data: {len(train_data)} samples, time range: {train_data['time_idx'].min()} to {train_data['time_idx'].max()}\")\n",
    "print(f\"Validation data: {len(val_data)} samples, time range: {val_data['time_idx'].min()} to {val_data['time_idx'].max()}\")\n",
    "print(f\"Test data: {len(test_data)} samples, time range: {test_data['time_idx'].min()} to {test_data['time_idx'].max()}\")\n",
    "\n",
    "# 9. Save processed data for model training\n",
    "train_data.to_csv('../data/train_data.csv', index=False)\n",
    "val_data.to_csv('../data/val_data.csv', index=False)\n",
    "test_data.to_csv('../data/test_data.csv', index=False)\n",
    "\n",
    "# Save feature definitions for TFT model\n",
    "feature_config = {\n",
    "    'static_features': static_features,\n",
    "    'time_varying_known_features': time_varying_known_features + ['quarter_sin', 'quarter_cos', 'festival_count'],\n",
    "    'time_varying_unknown_features': time_varying_unknown_features + ['sales_momentum', 'distributor_quarter_avg', 'product_quarter_avg'],\n",
    "    'target': target,\n",
    "    'time_idx': 'time_idx',\n",
    "    'entity_id': 'entity_id'\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/feature_config.json', 'w') as f:\n",
    "    json.dump(feature_config, f)\n",
    "\n",
    "print(\"\\n--- Feature Engineering Complete ---\")\n",
    "print(\"Processed data saved to data folder\")\n",
    "print(\"Feature configuration saved to data/feature_config.json\")\n",
    "\n",
    "# 10. Additional Visualizations of Engineered Features\n",
    "\n",
    "# Plot distribution of engineered features\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(sales_data['sales_momentum'].dropna(), kde=True)\n",
    "plt.title('Sales Momentum Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(sales_data['distributor_quarter_avg'], kde=True)\n",
    "plt.title('Distributor Quarter Average Sales Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(sales_data['product_quarter_avg'], kde=True)\n",
    "plt.title('Product Quarter Average Sales Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.countplot(x='festival_count', data=sales_data)\n",
    "plt.title('Festival Count Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show example of time series for one entity\n",
    "example_entity = sales_data['entity_id'].iloc[0]\n",
    "example_data = sales_data[sales_data['entity_id'] == example_entity].sort_values('time_idx')\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(example_data['time_idx'], example_data['sales'], marker='o', label='Sales')\n",
    "plt.plot(example_data['time_idx'], example_data['prev_quarter_sales'], marker='x', label='Previous Quarter Sales')\n",
    "plt.fill_between(example_data['time_idx'], 0, example_data['festival_count'], alpha=0.3, color='green', label='Festival Count')\n",
    "plt.title(f'Time Series for Entity: {example_entity}')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Scaled Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
