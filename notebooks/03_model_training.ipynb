{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Loading data and model...\")\n",
    "# Load test data\n",
    "test_data = pd.read_csv('../data/test_data.csv')\n",
    "train_data = pd.read_csv('../data/train_data.csv')\n",
    "val_data = pd.read_csv('../data/val_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load original data for reference\n",
    "original_data = pd.read_csv('../sales_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model metadata\n",
    "with open('../models/model_metadata.json', 'r') as f:\n",
    "    model_metadata = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load scaler to reverse transformations\n",
    "scaler = joblib.load('../models/feature_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load feature config\n",
    "with open('../data/feature_config.json', 'r') as f:\n",
    "    feature_config = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create test dataset with the same parameters as training\n",
    "test_dataset = TimeSeriesDataSet(\n",
    "    data=test_data,\n",
    "    time_idx=model_metadata[\"time_idx\"],\n",
    "    target=model_metadata[\"target\"],\n",
    "    group_ids=model_metadata[\"group_ids\"],\n",
    "    max_encoder_length=model_metadata[\"max_encoder_length\"],\n",
    "    max_prediction_length=model_metadata[\"max_prediction_length\"],\n",
    "    static_categoricals=model_metadata[\"static_categoricals\"],\n",
    "    static_reals=model_metadata[\"static_reals\"],\n",
    "    time_varying_known_categoricals=model_metadata[\"time_varying_known_categoricals\"],\n",
    "    time_varying_known_reals=model_metadata[\"time_varying_known_reals\"],\n",
    "    time_varying_unknown_categoricals=model_metadata[\"time_varying_unknown_categoricals\"],\n",
    "    time_varying_unknown_reals=model_metadata[\"time_varying_unknown_reals\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataloader = test_dataset.to_dataloader(train=False, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load best model\n",
    "print(\"Loading trained model...\")\n",
    "try:\n",
    "    best_model_path = \"../models/checkpoints/tft-sales-forecasting-best.ckpt\"\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Using saved model state instead of checkpoint...\")\n",
    "    # Initialize model architecture\n",
    "    training = TimeSeriesDataSet(\n",
    "        data=train_data,\n",
    "        time_idx=model_metadata[\"time_idx\"],\n",
    "        target=model_metadata[\"target\"],\n",
    "        group_ids=model_metadata[\"group_ids\"],\n",
    "        max_encoder_length=model_metadata[\"max_encoder_length\"],\n",
    "        max_prediction_length=model_metadata[\"max_prediction_length\"],\n",
    "        static_categoricals=model_metadata[\"static_categoricals\"],\n",
    "        static_reals=model_metadata[\"static_reals\"],\n",
    "        time_varying_known_categoricals=model_metadata[\"time_varying_known_categoricals\"],\n",
    "        time_varying_known_reals=model_metadata[\"time_varying_known_reals\"],\n",
    "        time_varying_unknown_categoricals=model_metadata[\"time_varying_unknown_categoricals\"],\n",
    "        time_varying_unknown_reals=model_metadata[\"time_varying_unknown_reals\"],\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "    best_tft = TemporalFusionTransformer.from_dataset(training)\n",
    "    # Load saved weights\n",
    "    best_tft.load_state_dict(torch.load('../models/tft_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Making predictions...\")\n",
    "# Make predictions\n",
    "predictions = best_tft.predict(test_dataloader, return_x=True, return_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract actual and predicted values\n",
    "x, y = predictions.x, predictions.y\n",
    "prediction_values = predictions.output.detach().cpu().numpy().flatten()\n",
    "actual_values = y[0].detach().cpu().numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataFrame with predictions and actuals\n",
    "results = pd.DataFrame({\n",
    "    'actual': actual_values,\n",
    "    'predicted': prediction_values\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inverse transform if needed (assuming sales was standardized)\n",
    "# Extract column indices for numeric features\n",
    "orig_numeric_features = ['prev_quarter_sales', 'total_quarter_sales', 'avg_quarterly_sales', 'sales']\n",
    "scaled_cols_idx = {col: i for i, col in enumerate(orig_numeric_features)}\n",
    "sales_idx = scaled_cols_idx.get('sales', 0)  # Default to 0 if not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create arrays to inverse transform\n",
    "actual_scaled = np.zeros((len(results), len(orig_numeric_features)))\n",
    "actual_scaled[:, sales_idx] = results['actual']\n",
    "\n",
    "pred_scaled = np.zeros((len(results), len(orig_numeric_features)))\n",
    "pred_scaled[:, sales_idx] = results['predicted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inverse transform\n",
    "actual_unscaled = scaler.inverse_transform(actual_scaled)[:, sales_idx]\n",
    "pred_unscaled = scaler.inverse_transform(pred_scaled)[:, sales_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add to results DataFrame\n",
    "results['actual_unscaled'] = actual_unscaled\n",
    "results['predicted_unscaled'] = pred_unscaled\n",
    "results['prediction_error'] = results['actual_unscaled'] - results['predicted_unscaled']\n",
    "results['percentage_error'] = (results['prediction_error'] / results['actual_unscaled']) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get test data indices\n",
    "idx = list(x[\"input_ids\"].detach().cpu().numpy().flatten())\n",
    "test_indices = [i for i, val in enumerate(idx) if val != -100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add entity information\n",
    "results['entity_id'] = test_data.iloc[test_indices]['entity_id'].values\n",
    "results['time_idx'] = test_data.iloc[test_indices]['time_idx'].values\n",
    "results['distributor_id'] = test_data.iloc[test_indices]['distributor_id'].values\n",
    "results['sku'] = test_data.iloc[test_indices]['sku'].values\n",
    "results['quarter'] = test_data.iloc[test_indices]['quarter'].values\n",
    "results['year'] = test_data.iloc[test_indices]['year'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate metrics\n",
    "mae = MAE()(torch.tensor(results['predicted_unscaled'].values), torch.tensor(results['actual_unscaled'].values))\n",
    "rmse = RMSE()(torch.tensor(results['predicted_unscaled'].values), torch.tensor(results['actual_unscaled'].values))\n",
    "smape = SMAPE()(torch.tensor(results['predicted_unscaled'].values), torch.tensor(results['actual_unscaled'].values))\n",
    "mape = np.mean(np.abs(results['percentage_error'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print metrics\n",
    "print(\"\\n--- Model Performance Metrics ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Symmetric Mean Absolute Percentage Error (SMAPE): {smape:.2f}%\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization of predictions vs actuals\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(results['actual_unscaled'], results['predicted_unscaled'], alpha=0.5)\n",
    "plt.plot([min(results['actual_unscaled']), max(results['actual_unscaled'])], \n",
    "         [min(results['actual_unscaled']), max(results['actual_unscaled'])], \n",
    "         'r--')\n",
    "plt.xlabel('Actual Sales')\n",
    "plt.ylabel('Predicted Sales')\n",
    "plt.title('Predicted vs Actual Sales')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Distribution of prediction errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(results['prediction_error'], bins=30, alpha=0.75)\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate MAPE by category\n",
    "category_errors = results.join(test_data.iloc[test_indices][['category']], how='left')\n",
    "category_mape = category_errors.groupby('category')['percentage_error'].apply(lambda x: np.mean(np.abs(x))).sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "category_mape.plot(kind='bar')\n",
    "plt.title('Mean Absolute Percentage Error by Product Category')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate MAPE by distributor\n",
    "distributor_mape = category_errors.groupby('distributor_id')['percentage_error'].apply(lambda x: np.mean(np.abs(x))).sort_values()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "distributor_mape[:15].plot(kind='bar')\n",
    "plt.title('Mean Absolute Percentage Error by Top 15 Distributors')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze festival impact on prediction accuracy\n",
    "festival_columns = ['is_diwali', 'is_ganesh_chaturthi', 'is_gudi_padwa', 'is_eid', \n",
    "                   'is_akshay_tritiya', 'is_dussehra_navratri', 'is_onam', 'is_christmas']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create extended results with festival data\n",
    "extended_results = results.join(test_data.iloc[test_indices][festival_columns], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate MAPE for festival and non-festival periods\n",
    "extended_results['has_festival'] = extended_results[festival_columns].sum(axis=1) > 0\n",
    "\n",
    "festival_mape = extended_results.groupby('has_festival')['percentage_error'].apply(lambda x: np.mean(np.abs(x)))\n",
    "print(\"\\n--- Festival Impact on Prediction Accuracy ---\")\n",
    "print(f\"MAPE during festival periods: {festival_mape.get(True, float('nan')):.2f}%\")\n",
    "print(f\"MAPE during non-festival periods: {festival_mape.get(False, float('nan')):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create specific festival MAPE\n",
    "festival_impact = {}\n",
    "for festival in festival_columns:\n",
    "    festival_rows = extended_results[extended_results[festival] == 1]\n",
    "    if len(festival_rows) > 0:\n",
    "        festival_impact[festival] = np.mean(np.abs(festival_rows['percentage_error']))\n",
    "    else:\n",
    "        festival_impact[festival] = float('nan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot festival impact\n",
    "plt.figure(figsize=(12, 6))\n",
    "impact_df = pd.DataFrame(list(festival_impact.items()), columns=['Festival', 'MAPE'])\n",
    "impact_df = impact_df.sort_values('MAPE')\n",
    "sns.barplot(x='Festival', y='MAPE', data=impact_df)\n",
    "plt.title('Prediction Error by Festival')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze model performance by movement category\n",
    "movement_mape = extended_results.join(test_data.iloc[test_indices][['movement_category']], how='left')\n",
    "movement_mape = movement_mape.groupby('movement_category')['percentage_error'].apply(lambda x: np.mean(np.abs(x)))\n",
    "\n",
    "print(\"\\n--- Prediction Accuracy by Movement Category ---\")\n",
    "for category, mape in movement_mape.items():\n",
    "    print(f\"MAPE for {category} items: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "movement_mape.plot(kind='bar')\n",
    "plt.title('Mean Absolute Percentage Error by Movement Category')\n",
    "plt.ylabel('MAPE (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate order recommendations\n",
    "print(\"\\n--- Sample Order Recommendations ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample 5 random entities\n",
    "sample_entities = results['entity_id'].sample(5).unique()\n",
    "\n",
    "for entity in sample_entities:\n",
    "    entity_data = results[results['entity_id'] == entity]\n",
    "    if len(entity_data) == 0:\n",
    "        continue\n",
    "        \n",
    "    distributor = entity_data['distributor_id'].iloc[0]\n",
    "    sku = entity_data['sku'].iloc[0]\n",
    "    \n",
    "    # Get historical data for context\n",
    "    historical = original_data[(original_data['distributor_id'] == distributor) & (original_data['sku'] == sku)]\n",
    "    \n",
    "    # Get the most recent prediction\n",
    "    recent = entity_data.sort_values('time_idx', ascending=False).iloc[0]\n",
    "    \n",
    "    # Calculate average historical sales for this distributor-SKU pair\n",
    "    avg_sales = historical['sales'].mean()\n",
    "    \n",
    "    # Get category information\n",
    "    try:\n",
    "        category = historical['category'].iloc[0]\n",
    "        movement = historical['movement_category'].iloc[0]\n",
    "    except:\n",
    "        category = \"Unknown\"\n",
    "        movement = \"Unknown\"\n",
    "    \n",
    "    # Create recommendation\n",
    "    predicted_sales = recent['predicted_unscaled']\n",
    "    if predicted_sales < 0:\n",
    "        predicted_sales = 0  # Ensure no negative predictions\n",
    "        \n",
    "    # Apply a safety buffer based on movement category\n",
    "    buffer_factor = 1.2 if movement == \"Fast Moving\" else 1.1 if movement == \"Medium\" else 1.05\n",
    "    recommendation = predicted_sales * buffer_factor\n",
    "    \n",
    "    print(f\"\\nDistributor: {distributor}, SKU: {sku}, Category: {category}, Movement: {movement}\")\n",
    "    print(f\"Next quarter predicted sales: {predicted_sales:.2f}\")\n",
    "    print(f\"Recommended order quantity: {recommendation:.2f}\")\n",
    "    print(f\"Average historical sales: {avg_sales:.2f}\")\n",
    "    \n",
    "    # Check if festival quarter\n",
    "    quarter = recent['quarter']\n",
    "    year = recent['year']\n",
    "    festival_cols = [col for col in festival_columns if col in historical.columns]\n",
    "    festival_quarter = historical[(historical['quarter'] == quarter) & (historical['year'] == year)][festival_cols].sum().sum() > 0\n",
    "    \n",
    "    if festival_quarter:\n",
    "        print(\"Note: Festival quarter detected! Consider additional inventory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save recommendations to CSV\n",
    "print(\"\\n--- Generating Full Recommendations ---\")\n",
    "\n",
    "# Create recommendations for all entities\n",
    "recommendations = []\n",
    "\n",
    "# Group by entity and find most recent prediction\n",
    "for entity, group in results.groupby('entity_id'):\n",
    "    recent = group.sort_values('time_idx', ascending=False).iloc[0]\n",
    "    \n",
    "    distributor = recent['distributor_id']\n",
    "    sku = recent['sku']\n",
    "    \n",
    "    # Get historical data\n",
    "    historical = original_data[(original_data['distributor_id'] == distributor) & (original_data['sku'] == sku)]\n",
    "    \n",
    "    # Get metadata\n",
    "    try:\n",
    "        category = historical['category'].iloc[0]\n",
    "        movement = historical['movement_category'].iloc[0]\n",
    "    except:\n",
    "        category = \"Unknown\"\n",
    "        movement = \"Unknown\"\n",
    "    \n",
    "    # Calculate predicted sales and recommendation\n",
    "    predicted_sales = max(0, recent['predicted_unscaled'])  # Ensure no negative predictions\n",
    "    \n",
    "    # Apply buffer based on movement category\n",
    "    buffer_factor = 1.2 if movement == \"Fast Moving\" else 1.1 if movement == \"Medium\" else 1.05\n",
    "    recommendation = predicted_sales * buffer_factor\n",
    "    \n",
    "    # Check if festival quarter\n",
    "    quarter = recent['quarter']\n",
    "    year = recent['year']\n",
    "    festival_cols = [col for col in festival_columns if col in historical.columns]\n",
    "    festival_quarter = historical[(historical['quarter'] == quarter) & (historical['year'] == year)][festival_cols].sum().sum() > 0\n",
    "    \n",
    "    # Add festival boost if needed\n",
    "    if festival_quarter:\n",
    "        recommendation *= 1.15  # Add 15% for festivals\n",
    "    \n",
    "    # Store recommendation\n",
    "    recommendations.append({\n",
    "        'distributor_id': distributor,\n",
    "        'sku': sku,\n",
    "        'category': category,\n",
    "        'movement_category': movement,\n",
    "        'quarter': quarter,\n",
    "        'year': year,\n",
    "        'predicted_sales': predicted_sales,\n",
    "        'recommended_order': recommendation,\n",
    "        'has_festival': festival_quarter\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame and save\n",
    "recommendations_df = pd.DataFrame(recommendations)\n",
    "recommendations_df.to_csv('../results/order_recommendations.csv', index=False)\n",
    "\n",
    "print(f\"Order recommendations generated for {len(recommendations_df)} distributor-SKU combinations\")\n",
    "print(\"Recommendations saved to '../results/order_recommendations.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show feature importance from TFT model\n",
    "print(\"\\n--- Feature Importance Analysis ---\")\n",
    "interpretation = best_tft.interpret_output(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Variable importance plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "best_tft.plot_variable_importance(interpretation)\n",
    "plt.title('TFT Model Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Attention plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "try:\n",
    "    interpretation2 = best_tft.interpret_output(test_dataloader, attention=True)\n",
    "    best_tft.plot_attention(interpretation2.attention)\n",
    "    plt.title('TFT Attention Mechanism')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except:\n",
    "    print(\"Unable to plot attention mechanism - may require retraining with additional parameters\")\n",
    "\n",
    "print(\"\\n--- Key Findings and Recommendations ---\")\n",
    "print(\"1. The model achieves an overall MAPE of {:.2f}%, indicating good accuracy for inventory planning\".format(mape))\n",
    "print(\"2. Performance varies across product categories with best predictions for: \" + \n",
    "      \", \".join(category_mape.nsmallest(3).index.tolist()))\n",
    "print(\"3. Festival periods impact prediction accuracy - special consideration needed\")\n",
    "print(\"4. Different buffer strategies recommended based on movement category\")\n",
    "print(\"5. Order recommendations incorporate both prediction and category-specific safety factors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by entity and find most recent prediction\n",
    "for entity, group in results.groupby('entity_id'):\n",
    "    recent = group.sort_values('time_idx', ascending=False).iloc[0]\n",
    "    \n",
    "    distributor = recent['distributor_id']\n",
    "    sku = recent['sku']\n",
    "    \n",
    "    # Get historical data\n",
    "    historical = original_data[(original_data['distributor_id'] == distributor) & (original_data['sku'] == sku)]\n",
    "    \n",
    "    # Get metadata\n",
    "    try:\n",
    "        category = historical['category'].iloc[0]\n",
    "        movement = historical['movement_category'].iloc[0]\n",
    "    except:\n",
    "        category = \"Unknown\"\n",
    "        movement = \"Unknown\"\n",
    "    \n",
    "    # Calculate predicted sales and recommendation\n",
    "    predicted_sales = max(0, recent['predicted_unscaled'])  # Ensure no negative predictions\n",
    "    \n",
    "    # Apply buffer based on movement category\n",
    "    buffer_factor = 1.2 if movement == \"Fast Moving\" else 1.1 if movement == \"Medium\" else 1.05\n",
    "    recommendation = predicted_sales * buffer_factor\n",
    "    \n",
    "    # Check if festival quarter\n",
    "    quarter = recent['quarter']\n",
    "    year = recent['year']\n",
    "    festival_cols = [col for col in festival_columns if col in historical.columns]\n",
    "    festival_quarter = historical[(historical['quarter'] == quarter) & (historical['year'] == year)][festival_cols].sum().sum() > 0\n",
    "    \n",
    "    # Add festival boost if needed\n",
    "    if festival_quarter:\n",
    "        recommendation *= 1.15  # Add 15% for festivals\n",
    "    \n",
    "    # Store recommendation\n",
    "    recommendations.append({\n",
    "        'distributor_id': distributor,\n",
    "        'sku': sku,\n",
    "        'category': category,\n",
    "        'movement_category': movement,\n",
    "        'quarter': quarter,\n",
    "        'year': year,\n",
    "        'predicted_sales': predicted_sales,\n",
    "        'recommended_order': recommendation,\n",
    "        'has_festival': festival_quarter\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame and save\n",
    "recommendations_df = pd.DataFrame(recommendations)\n",
    "recommendations_df.to_csv('../results/order_recommendations.csv', index=False)\n",
    "\n",
    "print(f\"Order recommendations generated for {len(recommendations_df)} distributor-SKU combinations\")\n",
    "print(\"Recommendations saved to '../results/order_recommendations.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show feature importance from TFT model\n",
    "print(\"\\n--- Feature Importance Analysis ---\")\n",
    "interpretation = best_tft.interpret_output(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Variable importance plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "best_tft.plot_variable_importance(interpretation)\n",
    "plt.title('TFT Model Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attention plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "try:\n",
    "    interpretation2 = best_tft.interpret_output(test_dataloader, attention=True)\n",
    "    best_tft.plot_attention(interpretation2.attention)\n",
    "    plt.title('TFT Attention Mechanism')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except:\n",
    "    print(\"Unable to plot attention mechanism - may require retraining with additional parameters\")\n",
    "\n",
    "print(\"\\n--- Key Findings and Recommendations ---\")\n",
    "print(\"1. The model achieves an overall MAPE of {:.2f}%, indicating good accuracy for inventory planning\".format(mape))\n",
    "print(\"2. Performance varies across product categories with best predictions for: \" + \n",
    "      \", \".join(category_mape.nsmallest(3).index.tolist()))\n",
    "print(\"3. Festival periods impact prediction accuracy - special consideration needed\")\n",
    "print(\"4. Different buffer strategies recommended based on movement category\")\n",
    "print(\"5. Order recommendations incorporate both prediction and category-specific safety factors\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
